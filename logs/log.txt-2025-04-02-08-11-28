***************
** Arguments **
***************
agnostic: True
algorithm: tabu
api_idx: 0
backbone: tlite
batch_size: 4
budget: 1000
checkpoint_freq: 10
classification_task_ids: ['019', '001', '022', '050', '069', '137', '139', '195']
config_file: 
data_dir: ./data/
data_seed: 42
dataset_config_file: 
edits: ['del', 'swap', 'sub', 'add']
eval_only: False
key_id: 0
level: phrase
load_epoch: None
meta_dir: ./logs/
meta_name: TS_batchsize_4_all_edits_l_1_m_8_n_20@task_001_agnostic_trainseed_0_seed_42_rho_7.txt
mode: Instruction Only
model_dir: 
model_name: text-babbage-001
no_train: False
num_candidates: 8
num_compose: 1
num_iter: 3
num_samples: 100
num_shots: 2
num_train: 100
opts: []
output_dir: ./output/
patience: 5
print_orig: True
project_name: ts-prompt
resume: 
simulated_anneal: False
task_idx: 2
tournament_selection: 4
train_seed: 0
transforms: None
write_preds: True
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 4
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 32
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: 
  NUM_LABELED: -1
  NUM_SHOTS: -1
  ROOT: 
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: all
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
DATA_SEED: 42
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bilinear
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.485, 0.456, 0.406]
  PIXEL_STD: [0.229, 0.224, 0.225]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  ROTATE_ANGLE_LIST: [0]
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ()
META_DIR: ./logs/
MODEL:
  BACKBONE:
    NAME: 
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  GAMMA: 0.1
  GRAD_ACCU_STEPS: 8
  LR: 0.0003
  LR_SCHEDULER: single_step
  MAX_EPOCH: 10
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: -1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: linear
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: ./output/
RESUME: 
SEED: -1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 10
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  GA:
    PREC: fp16
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: 
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
TRAIN_SEED: 0
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
./data/
dict_keys(['task022'])
dict_values(['task022_cosmosqa_passage_inappropriate_binary.json'])
Chosen Task:  022
Running Experiment for:  task022_cosmosqa_passage_inappropriate_binary.json
Task Labels:  ['No', 'Yes']
Original Instruction:  ['Read the given context and if the the context is inappropriate (e.g., pornographic) or nonsensical (e.g., cannot determine what happenings the context is about), indicate via "yes". Otherwise, response via "no".']
Instruction Edit1:  ['Read the given context and if the the context is inappropriate (e.g., pornographic) or nonsensical (e.g., cannot determine what happenings the context is about), indicate via "yes". Otherwise, response via "no".']
Instruction Edit2:  Read the given context and if the the context is inappropriate (e.g., pornographic) or nonsensical (e.g., cannot determine what happenings the context is about), indicate via "yes". Otherwise, response via "no".
Sanity Check, Task Labels are:  [' No', ' Yes']
generating sequence for prompts len 4: ["Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: If you 're wondering why he had n't done this already , it 's because work has been extremely busy recently and there has n't been time . So , that brings me back to the girls and their bugs . We caught several toads in the grass while watching Dad on the roof . They had a great time holding them , watching them jump , etc .\noutput:", "Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: I already called the company and they said they will come check it out but Hurricane Ike caused a lot of damage so a lot of people already called so they are expected to come check it in weeks . We already tried to do everything they told us to do like plug it in and out and restart the system but nothing helped ! We even went on the roof and made sure all the wires and everything were secure and the dish was perfect . We checked if the wires in the attic were secure too .\noutput:", "Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: Jangan cuba nak tipu kita la ! Oi you ingat kita bodoh ka ! ? ? ! ! Wah I tell you , if can I wanted to go down and kick down his door and beat him up lo . But can not la , since I 'm only like 45 kg and he 's this rough Indian taxi driver ( altho I think at that point I was more cho lo than him la heh ) .\noutput:", "Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: Worse still , when both of them rings ! Which one am I suppose to pick up ? We have one of those headsets thingy for the phones , but I had no idea how to use it . I always thought it was pretty cool .\noutput:"]
seq RequestOutput(request_id=0, prompt="Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: If you 're wondering why he had n't done this already , it 's because work has been extremely busy recently and there has n't been time . So , that brings me back to the girls and their bugs . We caught several toads in the grass while watching Dad on the roof . They had a great time holding them , watching them jump , etc .\noutput:", prompt_token_ids=[10614, 25, 1472, 690, 387, 2728, 264, 3465, 662, 4557, 323, 3619, 279, 3465, 15884, 11, 323, 36001, 4320, 364, 2822, 6, 477, 364, 9642, 24482, 1379, 25, 9805, 25, 1442, 499, 364, 265, 20910, 3249, 568, 1047, 308, 956, 2884, 420, 2736, 1174, 433, 364, 82, 1606, 990, 706, 1027, 9193, 13326, 6051, 323, 1070, 706, 308, 956, 1027, 892, 662, 2100, 1174, 430, 12716, 757, 1203, 311, 279, 7724, 323, 872, 23367, 662, 1226, 10791, 3892, 311, 7819, 304, 279, 16763, 1418, 10307, 33621, 389, 279, 15485, 662, 2435, 1047, 264, 2294, 892, 10168, 1124, 1174, 10307, 1124, 7940, 1174, 5099, 16853, 3081, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' No', token_ids=(2360,), cumulative_logprob=-1.0403560400009155, logprobs=[{2360: Logprob(logprob=-1.0403560400009155, rank=1, decoded_token=' No'), 7566: Logprob(logprob=-2.313793659210205, rank=2, decoded_token=' Yes'), 2209: Logprob(logprob=-2.876293659210205, rank=3, decoded_token=' Is'), 12838: Logprob(logprob=-3.204418659210205, rank=4, decoded_token=' Does'), 578: Logprob(logprob=-3.321606159210205, rank=5, decoded_token=' The'), 3639: Logprob(logprob=-3.563793659210205, rank=6, decoded_token=' What'), 8886: Logprob(logprob=-3.563793659210205, rank=7, decoded_token=' Are'), 14910: Logprob(logprob=-3.720043659210205, rank=8, decoded_token=' Did'), 1226: Logprob(logprob=-3.923168659210205, rank=9, decoded_token=' We'), 358: Logprob(logprob=-3.930981159210205, rank=10, decoded_token=' I'), 8595: Logprob(logprob=-4.001293659210205, rank=11, decoded_token=' Why'), 3234: Logprob(logprob=-4.063793659210205, rank=12, decoded_token=' Do'), 2684: Logprob(logprob=-4.305981159210205, rank=13, decoded_token=' There'), 1102: Logprob(logprob=-4.430981159210205, rank=14, decoded_token=' It'), 3053: Logprob(logprob=-4.470043659210205, rank=15, decoded_token=' Can'), 2100: Logprob(logprob=-4.555981159210205, rank=16, decoded_token=' So'), 11697: Logprob(logprob=-4.571606159210205, rank=17, decoded_token=' Has'), 720: Logprob(logprob=-4.766918659210205, rank=18, decoded_token=' \n'), 12522: Logprob(logprob=-4.798168659210205, rank=19, decoded_token=' Have'), 1442: Logprob(logprob=-4.805981159210205, rank=20, decoded_token=' If')}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1743570697.785086, last_token_time=1743570697.924059, first_scheduled_time=1743570697.7897701, first_token_time=1743570697.924059, time_in_queue=0.0046842098236083984, finished_time=1743570697.9251018, scheduler_time=0.0016680430271662772, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
seq RequestOutput(request_id=1, prompt="Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: I already called the company and they said they will come check it out but Hurricane Ike caused a lot of damage so a lot of people already called so they are expected to come check it in weeks . We already tried to do everything they told us to do like plug it in and out and restart the system but nothing helped ! We even went on the roof and made sure all the wires and everything were secure and the dish was perfect . We checked if the wires in the attic were secure too .\noutput:", prompt_token_ids=[10614, 25, 1472, 690, 387, 2728, 264, 3465, 662, 4557, 323, 3619, 279, 3465, 15884, 11, 323, 36001, 4320, 364, 2822, 6, 477, 364, 9642, 24482, 1379, 25, 9805, 25, 358, 2736, 2663, 279, 2883, 323, 814, 1071, 814, 690, 2586, 1817, 433, 704, 719, 38201, 99734, 9057, 264, 2763, 315, 5674, 779, 264, 2763, 315, 1274, 2736, 2663, 779, 814, 527, 3685, 311, 2586, 1817, 433, 304, 5672, 662, 1226, 2736, 6818, 311, 656, 4395, 814, 3309, 603, 311, 656, 1093, 20206, 433, 304, 323, 704, 323, 17460, 279, 1887, 719, 4400, 9087, 758, 1226, 1524, 4024, 389, 279, 15485, 323, 1903, 2771, 682, 279, 36108, 323, 4395, 1051, 9966, 323, 279, 12269, 574, 4832, 662, 1226, 10273, 422, 279, 36108, 304, 279, 74721, 1051, 9966, 2288, 16853, 3081, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' No', token_ids=(2360,), cumulative_logprob=-1.157371163368225, logprobs=[{2360: Logprob(logprob=-1.157371163368225, rank=1, decoded_token=' No'), 7566: Logprob(logprob=-2.4698710441589355, rank=2, decoded_token=' Yes'), 2209: Logprob(logprob=-2.7979960441589355, rank=3, decoded_token=' Is'), 3639: Logprob(logprob=-3.1495585441589355, rank=4, decoded_token=' What'), 14910: Logprob(logprob=-3.1729960441589355, rank=5, decoded_token=' Did'), 8886: Logprob(logprob=-3.2979960441589355, rank=6, decoded_token=' Are'), 12838: Logprob(logprob=-3.3761210441589355, rank=7, decoded_token=' Does'), 38201: Logprob(logprob=-3.6495585441589355, rank=8, decoded_token=' Hurricane'), 4946: Logprob(logprob=-3.7276835441589355, rank=9, decoded_token=' Will'), 3234: Logprob(logprob=-3.8761210441589355, rank=10, decoded_token=' Do'), 12522: Logprob(logprob=-3.8917460441589355, rank=11, decoded_token=' Have'), 578: Logprob(logprob=-3.9854960441589355, rank=12, decoded_token=' The'), 358: Logprob(logprob=-4.0792460441589355, rank=13, decoded_token=' I'), 1226: Logprob(logprob=-4.2276835441589355, rank=14, decoded_token=' We'), 720: Logprob(logprob=-4.3761210441589355, rank=15, decoded_token=' \n'), 2650: Logprob(logprob=-4.4854960441589355, rank=16, decoded_token=' How'), 8595: Logprob(logprob=-4.6104960441589355, rank=17, decoded_token=' Why'), 11697: Logprob(logprob=-4.6886210441589355, rank=18, decoded_token=' Has'), 15148: Logprob(logprob=-4.7667460441589355, rank=19, decoded_token=' Was'), 16225: Logprob(logprob=-4.9151835441589355, rank=20, decoded_token=' Question')}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1743570697.7866411, last_token_time=1743570697.924059, first_scheduled_time=1743570697.7897701, first_token_time=1743570697.924059, time_in_queue=0.0031290054321289062, finished_time=1743570697.925129, scheduler_time=0.0016680430271662772, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
seq RequestOutput(request_id=2, prompt="Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: Jangan cuba nak tipu kita la ! Oi you ingat kita bodoh ka ! ? ? ! ! Wah I tell you , if can I wanted to go down and kick down his door and beat him up lo . But can not la , since I 'm only like 45 kg and he 's this rough Indian taxi driver ( altho I think at that point I was more cho lo than him la heh ) .\noutput:", prompt_token_ids=[10614, 25, 1472, 690, 387, 2728, 264, 3465, 662, 4557, 323, 3619, 279, 3465, 15884, 11, 323, 36001, 4320, 364, 2822, 6, 477, 364, 9642, 24482, 1379, 25, 9805, 25, 622, 19148, 19250, 64, 41986, 11813, 84, 55311, 1208, 758, 507, 72, 499, 6892, 266, 55311, 30111, 2319, 16909, 758, 949, 949, 758, 758, 70690, 358, 3371, 499, 1174, 422, 649, 358, 4934, 311, 733, 1523, 323, 10536, 1523, 813, 6134, 323, 9567, 1461, 709, 781, 662, 2030, 649, 539, 1208, 1174, 2533, 358, 364, 76, 1193, 1093, 220, 1774, 21647, 323, 568, 364, 82, 420, 11413, 7904, 33605, 5696, 320, 453, 339, 78, 358, 1781, 520, 430, 1486, 358, 574, 810, 2665, 781, 1109, 1461, 1208, 568, 71, 883, 16853, 3081, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' No', token_ids=(2360,), cumulative_logprob=-1.0641778707504272, logprobs=[{2360: Logprob(logprob=-1.0641778707504272, rank=1, decoded_token=' No'), 7566: Logprob(logprob=-1.5407403707504272, rank=2, decoded_token=' Yes'), 2209: Logprob(logprob=-3.353240489959717, rank=3, decoded_token=' Is'), 358: Logprob(logprob=-3.728240489959717, rank=4, decoded_token=' I'), 3639: Logprob(logprob=-3.751677989959717, rank=5, decoded_token=' What'), 12838: Logprob(logprob=-3.853240489959717, rank=6, decoded_token=' Does'), 14910: Logprob(logprob=-3.876677989959717, rank=7, decoded_token=' Did'), 16225: Logprob(logprob=-4.095427989959717, rank=8, decoded_token=' Question'), 8886: Logprob(logprob=-4.157927989959717, rank=9, decoded_token=' Are'), 578: Logprob(logprob=-4.329802989959717, rank=10, decoded_token=' The'), 3053: Logprob(logprob=-4.486052989959717, rank=11, decoded_token=' Can'), 720: Logprob(logprob=-4.579802989959717, rank=12, decoded_token=' \n'), 3234: Logprob(logprob=-4.603240489959717, rank=13, decoded_token=' Do'), 1472: Logprob(logprob=-4.689177989959717, rank=14, decoded_token=' You'), 1102: Logprob(logprob=-5.134490489959717, rank=15, decoded_token=' It'), 2650: Logprob(logprob=-5.134490489959717, rank=16, decoded_token=' How'), 220: Logprob(logprob=-5.204802989959717, rank=17, decoded_token=' '), 8595: Logprob(logprob=-5.220427989959717, rank=18, decoded_token=' Why'), 912: Logprob(logprob=-5.259490489959717, rank=19, decoded_token=' no'), 362: Logprob(logprob=-5.259490489959717, rank=20, decoded_token=' A')}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1743570697.7874372, last_token_time=1743570697.924059, first_scheduled_time=1743570697.7897701, first_token_time=1743570697.924059, time_in_queue=0.002332925796508789, finished_time=1743570697.9251425, scheduler_time=0.0016680430271662772, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
seq RequestOutput(request_id=3, prompt="Definition: You will be given a task . Read and understand the task carefully, and appropriately answer 'No' or 'Yes'.\ninput: Context: Worse still , when both of them rings ! Which one am I suppose to pick up ? We have one of those headsets thingy for the phones , but I had no idea how to use it . I always thought it was pretty cool .\noutput:", prompt_token_ids=[10614, 25, 1472, 690, 387, 2728, 264, 3465, 662, 4557, 323, 3619, 279, 3465, 15884, 11, 323, 36001, 4320, 364, 2822, 6, 477, 364, 9642, 24482, 1379, 25, 9805, 25, 87904, 2103, 1174, 994, 2225, 315, 1124, 25562, 758, 16299, 832, 1097, 358, 23289, 311, 3820, 709, 949, 1226, 617, 832, 315, 1884, 2010, 5022, 3245, 88, 369, 279, 18084, 1174, 719, 358, 1047, 912, 4623, 1268, 311, 1005, 433, 662, 358, 2744, 3463, 433, 574, 5128, 7155, 16853, 3081, 25], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' No', token_ids=(2360,), cumulative_logprob=-0.6034558415412903, logprobs=[{2360: Logprob(logprob=-0.6034558415412903, rank=1, decoded_token=' No'), 7566: Logprob(logprob=-1.2284557819366455, rank=2, decoded_token=' Yes'), 3639: Logprob(logprob=-3.9472057819366455, rank=3, decoded_token=' What'), 720: Logprob(logprob=-4.345643520355225, rank=4, decoded_token=' \n'), 12838: Logprob(logprob=-4.509706020355225, rank=5, decoded_token=' Does'), 2209: Logprob(logprob=-4.689393520355225, rank=6, decoded_token=' Is'), 16225: Logprob(logprob=-5.189393520355225, rank=7, decoded_token=' Question'), 358: Logprob(logprob=-5.228456020355225, rank=8, decoded_token=' I'), 8886: Logprob(logprob=-5.322206020355225, rank=9, decoded_token=' Are'), 2650: Logprob(logprob=-5.455018520355225, rank=10, decoded_token=' How'), 3234: Logprob(logprob=-5.595643520355225, rank=11, decoded_token=' Do'), 3053: Logprob(logprob=-5.658143520355225, rank=12, decoded_token=' Can'), 14910: Logprob(logprob=-5.673768520355225, rank=13, decoded_token=' Did'), 8595: Logprob(logprob=-5.790956020355225, rank=14, decoded_token=' Why'), 912: Logprob(logprob=-5.978456020355225, rank=15, decoded_token=' no'), 578: Logprob(logprob=-6.064393520355225, rank=16, decoded_token=' The'), 1472: Logprob(logprob=-6.220643520355225, rank=17, decoded_token=' You'), 220: Logprob(logprob=-6.275331020355225, rank=18, decoded_token=' '), 1102: Logprob(logprob=-6.376893520355225, rank=19, decoded_token=' It'), 16299: Logprob(logprob=-6.470643520355225, rank=20, decoded_token=' Which')}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1743570697.7881687, last_token_time=1743570697.924059, first_scheduled_time=1743570697.7897701, first_token_time=1743570697.924059, time_in_queue=0.0016014575958251953, finished_time=1743570697.925155, scheduler_time=0.0016680430271662772, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
logprobs torch.Size([4, 1, 20]) tensor([[[-1.0404, -2.3138, -2.8763, -3.2044, -3.3216, -3.5638, -3.5638,
          -3.7200, -3.9232, -3.9310, -4.0013, -4.0638, -4.3060, -4.4310,
          -4.4700, -4.5560, -4.5716, -4.7669, -4.7982, -4.8060]],

        [[-1.1574, -2.4699, -2.7980, -3.1496, -3.1730, -3.2980, -3.3761,
          -3.6496, -3.7277, -3.8761, -3.8917, -3.9855, -4.0792, -4.2277,
          -4.3761, -4.4855, -4.6105, -4.6886, -4.7667, -4.9152]],

        [[-1.0642, -1.5407, -3.3532, -3.7282, -3.7517, -3.8532, -3.8767,
          -4.0954, -4.1579, -4.3298, -4.4861, -4.5798, -4.6032, -4.6892,
          -5.1345, -5.1345, -5.2048, -5.2204, -5.2595, -5.2595]],

        [[-0.6035, -1.2285, -3.9472, -4.3456, -4.5097, -4.6894, -5.1894,
          -5.2285, -5.3222, -5.4550, -5.5956, -5.6581, -5.6738, -5.7910,
          -5.9785, -6.0644, -6.2206, -6.2753, -6.3769, -6.4706]]])
complete_tlite has been used: 1x
